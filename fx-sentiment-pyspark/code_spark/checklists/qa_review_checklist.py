#!/usr/bin/env python3
"""
QA Review Checklist for Spark ML Baselines
Implements 5 automated verification checks with auto-fix capabilities
"""
import os
import sys
import re
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Any
import pandas as pd

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from code_spark.data_loader import (
    build_spark, load_first_available, sanitize_schema, get_feature_columns,
    split_timewise, print_split_summary
)

class QAResults:
    def __init__(self):
        self.results = {}
        self.evidence = {}
        self.fixes_applied = []
    
    def add_result(self, check_name: str, status: str, evidence: Dict[str, Any]):
        self.results[check_name] = status
        self.evidence[check_name] = evidence
    
    def add_fix(self, fix_description: str):
        self.fixes_applied.append(fix_description)
    
    def all_passed(self) -> bool:
        return all(status == "PASS" for status in self.results.values())

def assert_chronological_splits() -> Tuple[str, Dict[str, Any]]:
    """
    Check 1: Chronological data splitting
    Verify max(train.ts) < min(valid.ts) and max(valid.ts) < min(test.ts)
    """
    try:
        spark = build_spark("qa-chronological-check")
        
        paths = [
            "delta/silver/silver_eurusd_1h_features_noleak",
            "delta/silver/silver_eurusd_1h_features"
        ]
        df = load_first_available(spark, paths)
        df = sanitize_schema(df)
        
        from pyspark.sql.functions import when, col
        if "label" not in df.columns:
            df = df.withColumn("label", when(col("r_fwd1") > 0, 1).otherwise(0))
        else:
            df = df.withColumn("label", 
                when(col("label") == 1, 1)
                .otherwise(0)
            )
        
        train_df, valid_df, test_df = split_timewise(df)
        
        from pyspark.sql.functions import min as spark_min, max as spark_max
        
        train_count = train_df.count()
        valid_count = valid_df.count()
        test_count = test_df.count()
        
        if train_count == 0 or valid_count == 0 or test_count == 0:
            return "FAIL", {
                "error": "Empty splits detected",
                "train_count": train_count,
                "valid_count": valid_count,
                "test_count": test_count
            }
        
        train_max_ts = train_df.agg(spark_max("ts").alias("max_ts")).collect()[0]["max_ts"]
        valid_min_ts = valid_df.agg(spark_min("ts").alias("min_ts")).collect()[0]["min_ts"]
        valid_max_ts = valid_df.agg(spark_max("ts").alias("max_ts")).collect()[0]["max_ts"]
        test_min_ts = test_df.agg(spark_min("ts").alias("min_ts")).collect()[0]["min_ts"]
        
        chrono_check1 = train_max_ts < valid_min_ts
        chrono_check2 = valid_max_ts < test_min_ts
        
        evidence = {
            "train_count": train_count,
            "valid_count": valid_count,
            "test_count": test_count,
            "train_max_ts": str(train_max_ts),
            "valid_min_ts": str(valid_min_ts),
            "valid_max_ts": str(valid_max_ts),
            "test_min_ts": str(test_min_ts),
            "chronological_order_1": chrono_check1,
            "chronological_order_2": chrono_check2
        }
        
        spark.stop()
        
        if chrono_check1 and chrono_check2:
            return "PASS", evidence
        else:
            return "FAIL", evidence
            
    except Exception as e:
        return "FAIL", {"error": str(e)}

def check_feature_leakage() -> Tuple[str, Dict[str, Any]]:
    """
    Check 2: Feature leakage detection
    Ensure no forward-looking features and exactly 16 features remain
    """
    try:
        spark = build_spark("qa-feature-check")
        
        paths = [
            "delta/silver/silver_eurusd_1h_features_noleak",
            "delta/silver/silver_eurusd_1h_features"
        ]
        df = load_first_available(spark, paths)
        df = sanitize_schema(df)
        
        feature_cols = get_feature_columns(df)
        
        forward_patterns = [
            r"(^|_)fwd\d+",
            r"(^|_)lead\d+", 
            r"future",
            r"label($|_)",
            r"eps($|_)"
        ]
        
        leaky_features = []
        for feature in feature_cols:
            for pattern in forward_patterns:
                if re.search(pattern, feature, re.IGNORECASE):
                    leaky_features.append((feature, pattern))
        
        evidence = {
            "total_features": len(feature_cols),
            "feature_list": feature_cols,
            "leaky_features": leaky_features,
            "expected_count": 16
        }
        
        spark.stop()
        
        if leaky_features:
            return "FAIL", evidence
        elif len(feature_cols) != 16:
            return "FAIL", evidence
        else:
            return "PASS", evidence
            
    except Exception as e:
        return "FAIL", {"error": str(e)}

def run_e2e_pipeline() -> Tuple[str, Dict[str, Any]]:
    """
    Check 3: End-to-end pipeline execution
    Run training pipeline and verify artifacts are generated
    """
    try:
        os.makedirs("artifacts/logs", exist_ok=True)
        
        cmd = [
            "python", "-m", "scripts.spark_train_and_eval",
            "--plot-rows", "254"
        ]
        
        with open("artifacts/logs/train.log", "w") as log_file:
            result = subprocess.run(
                cmd,
                cwd="/home/ubuntu/CN7030---Hamad-Hussain/fx-sentiment-pyspark",
                capture_output=True,
                text=True,
                timeout=600  # 10 minute timeout
            )
            
            log_file.write("STDOUT:\n")
            log_file.write(result.stdout)
            log_file.write("\nSTDERR:\n")
            log_file.write(result.stderr)
            log_file.write(f"\nReturn code: {result.returncode}")
        
        if result.returncode != 0:
            return "FAIL", {
                "error": "Pipeline execution failed",
                "return_code": result.returncode,
                "stderr": result.stderr[:1000]  # First 1000 chars
            }
        
        artifacts_dir = Path("artifacts")
        plots_dir = artifacts_dir / "plots"
        
        required_files = [
            "artifacts/REPORT_BASELINES_SPARK.md",
            "artifacts/logs/train.log"
        ]
        
        missing_files = []
        existing_files = []
        
        for file_path in required_files:
            if Path(file_path).exists():
                existing_files.append(file_path)
            else:
                missing_files.append(file_path)
        
        plot_files = []
        if plots_dir.exists():
            plot_files = list(plots_dir.glob("*.png"))
        
        evidence = {
            "pipeline_success": result.returncode == 0,
            "existing_files": existing_files,
            "missing_files": missing_files,
            "plot_count": len(plot_files),
            "plot_files": [str(p) for p in plot_files[:10]]  # First 10 plots
        }
        
        if missing_files or len(plot_files) == 0:
            return "FAIL", evidence
        else:
            return "PASS", evidence
            
    except subprocess.TimeoutExpired:
        return "FAIL", {"error": "Pipeline execution timed out"}
    except Exception as e:
        return "FAIL", {"error": str(e)}

def validate_label_conversion() -> Tuple[str, Dict[str, Any]]:
    """
    Check 4: Label conversion validation
    Confirm 3-class ‚Üí binary mapping: {-1,0,1} ‚Üí {0,1} via 1‚Üí1, others ‚Üí 0
    """
    try:
        spark = build_spark("qa-label-check")
        
        paths = [
            "delta/silver/silver_eurusd_1h_features_noleak",
            "delta/silver/silver_eurusd_1h_features"
        ]
        df = load_first_available(spark, paths)
        
        original_label_col = None
        if "label_3cls" in df.columns:
            original_label_col = "label_3cls"
        elif "label" in df.columns:
            label_values = df.select("label").distinct().collect()
            label_set = {row["label"] for row in label_values}
            if label_set.intersection({-1, 0, 1}) and len(label_set) > 2:
                original_label_col = "label"
        
        from pyspark.sql.functions import when, col
        if "label" not in df.columns:
            df = df.withColumn("label", when(col("r_fwd1") > 0, 1).otherwise(0))
        else:
            df = df.withColumn("label", 
                when(col("label") == 1, 1)
                .otherwise(0)
            )
        
        label_counts = df.groupBy("label").count().collect()
        label_dist = {row["label"]: row["count"] for row in label_counts}
        
        valid_labels = set(label_dist.keys()).issubset({0, 1})
        
        train_df, valid_df, test_df = split_timewise(df)
        
        split_distributions = {}
        for split_name, split_df in [("train", train_df), ("valid", valid_df), ("test", test_df)]:
            split_counts = split_df.groupBy("label").count().collect()
            split_distributions[split_name] = {row["label"]: row["count"] for row in split_counts}
        
        evidence = {
            "original_label_column": original_label_col,
            "final_label_distribution": label_dist,
            "valid_binary_labels": valid_labels,
            "split_distributions": split_distributions
        }
        
        spark.stop()
        
        if valid_labels:
            return "PASS", evidence
        else:
            return "FAIL", evidence
            
    except Exception as e:
        return "FAIL", {"error": str(e)}

def review_evaluation_metrics() -> Tuple[str, Dict[str, Any]]:
    """
    Check 5: Evaluation metrics review
    Verify plots and metrics exist and look reasonable for small dataset
    """
    try:
        artifacts_dir = Path("artifacts")
        plots_dir = artifacts_dir / "plots"
        
        required_plot_patterns = [
            "roc_*.png",
            "pr_*.png", 
            "calibration_*.png"
        ]
        
        found_plots = {}
        missing_plots = []
        
        for pattern in required_plot_patterns:
            plot_type = pattern.split("_")[0]
            matching_plots = list(plots_dir.glob(pattern)) if plots_dir.exists() else []
            found_plots[plot_type] = [str(p) for p in matching_plots]
            if not matching_plots:
                missing_plots.append(pattern)
        
        report_file = artifacts_dir / "REPORT_BASELINES_SPARK.md"
        report_exists = report_file.exists()
        
        metrics_summary = {}
        if report_exists:
            try:
                with open(report_file, 'r') as f:
                    content = f.read()
                    
                if "Total rows:" in content:
                    total_rows_match = re.search(r"Total rows: (\d+)", content)
                    if total_rows_match:
                        metrics_summary["total_rows"] = int(total_rows_match.group(1))
                
                if "Features:" in content:
                    features_match = re.search(r"Features: (\d+)", content)
                    if features_match:
                        metrics_summary["features"] = int(features_match.group(1))
                
                if "Train/Valid/Test:" in content:
                    split_match = re.search(r"Train/Valid/Test: (\d+)/(\d+)/(\d+)", content)
                    if split_match:
                        metrics_summary["train_count"] = int(split_match.group(1))
                        metrics_summary["valid_count"] = int(split_match.group(2))
                        metrics_summary["test_count"] = int(split_match.group(3))
                        
            except Exception as e:
                metrics_summary["parse_error"] = str(e)
        
        mlruns_dir = Path("mlruns")
        mlflow_exists = mlruns_dir.exists()
        
        evidence = {
            "report_exists": report_exists,
            "mlflow_exists": mlflow_exists,
            "found_plots": found_plots,
            "missing_plots": missing_plots,
            "total_plot_count": sum(len(plots) for plots in found_plots.values()),
            "metrics_summary": metrics_summary
        }
        
        min_plots_per_type = 1
        sufficient_plots = all(len(plots) >= min_plots_per_type for plots in found_plots.values())
        
        if report_exists and sufficient_plots and not missing_plots:
            return "PASS", evidence
        else:
            return "FAIL", evidence
            
    except Exception as e:
        return "FAIL", {"error": str(e)}

def apply_auto_fixes(qa_results: QAResults) -> QAResults:
    """
    Apply auto-fixes for failing checks
    """
    
    for check_name, status in qa_results.results.items():
        if status == "FAIL":
            if check_name == "chronological_splits":
                qa_results.add_fix(f"Chronological splits: No auto-fix available - manual review needed")
                
            elif check_name == "feature_leakage":
                qa_results.add_fix(f"Feature leakage: Manual review of feature selection needed")
                
            elif check_name == "e2e_pipeline":
                qa_results.add_fix(f"E2E pipeline: Manual debugging required")
                
            elif check_name == "label_conversion":
                qa_results.add_fix(f"Label conversion: Manual review of label mapping needed")
                
            elif check_name == "evaluation_metrics":
                qa_results.add_fix(f"Evaluation metrics: Re-run pipeline to generate missing artifacts")
    
    return qa_results

def generate_qa_report(qa_results: QAResults) -> None:
    """
    Generate QA_REVIEW_REPORT.md with results and evidence
    """
    os.makedirs("artifacts", exist_ok=True)
    
    report_content = f"""# QA Review Report

- **Overall Status**: {'‚úÖ ALL PASS' if qa_results.all_passed() else '‚ùå SOME FAILURES'}
- **Checks Completed**: {len(qa_results.results)}
- **Fixes Applied**: {len(qa_results.fixes_applied)}


- **Status**: {qa_results.results.get('chronological_splits', 'NOT_RUN')}
- **Evidence**: 
  - Train count: {qa_results.evidence.get('chronological_splits', {}).get('train_count', 'N/A')}
  - Valid count: {qa_results.evidence.get('chronological_splits', {}).get('valid_count', 'N/A')}
  - Test count: {qa_results.evidence.get('chronological_splits', {}).get('test_count', 'N/A')}
  - Train max ts: {qa_results.evidence.get('chronological_splits', {}).get('train_max_ts', 'N/A')}
  - Valid min ts: {qa_results.evidence.get('chronological_splits', {}).get('valid_min_ts', 'N/A')}
  - Valid max ts: {qa_results.evidence.get('chronological_splits', {}).get('valid_max_ts', 'N/A')}
  - Test min ts: {qa_results.evidence.get('chronological_splits', {}).get('test_min_ts', 'N/A')}

- **Status**: {qa_results.results.get('feature_leakage', 'NOT_RUN')}
- **Evidence**:
  - Total features: {qa_results.evidence.get('feature_leakage', {}).get('total_features', 'N/A')}
  - Expected count: {qa_results.evidence.get('feature_leakage', {}).get('expected_count', 'N/A')}
  - Leaky features: {qa_results.evidence.get('feature_leakage', {}).get('leaky_features', 'N/A')}

- **Status**: {qa_results.results.get('e2e_pipeline', 'NOT_RUN')}
- **Evidence**:
  - Pipeline success: {qa_results.evidence.get('e2e_pipeline', {}).get('pipeline_success', 'N/A')}
  - Plot count: {qa_results.evidence.get('e2e_pipeline', {}).get('plot_count', 'N/A')}
  - Missing files: {qa_results.evidence.get('e2e_pipeline', {}).get('missing_files', 'N/A')}

- **Status**: {qa_results.results.get('label_conversion', 'NOT_RUN')}
- **Evidence**:
  - Valid binary labels: {qa_results.evidence.get('label_conversion', {}).get('valid_binary_labels', 'N/A')}
  - Label distribution: {qa_results.evidence.get('label_conversion', {}).get('final_label_distribution', 'N/A')}

- **Status**: {qa_results.results.get('evaluation_metrics', 'NOT_RUN')}
- **Evidence**:
  - Report exists: {qa_results.evidence.get('evaluation_metrics', {}).get('report_exists', 'N/A')}
  - Total plot count: {qa_results.evidence.get('evaluation_metrics', {}).get('total_plot_count', 'N/A')}
  - MLflow exists: {qa_results.evidence.get('evaluation_metrics', {}).get('mlflow_exists', 'N/A')}

"""
    
    if qa_results.fixes_applied:
        for i, fix in enumerate(qa_results.fixes_applied, 1):
            report_content += f"{i}. {fix}\n"
    else:
        report_content += "No fixes were applied.\n"
    
    report_content += f"""
- Training logs: `artifacts/logs/train.log`
- Plots directory: `artifacts/plots/`
- MLflow tracking: `./mlruns`
- This report: `artifacts/QA_REVIEW_REPORT.md`

Generated at: {pd.Timestamp.now()}
"""
    
    with open("artifacts/QA_REVIEW_REPORT.md", "w") as f:
        f.write(report_content)

def main():
    """
    Main QA review function - runs all checks and generates report
    """
    print("üîç Starting QA Review Checklist...")
    
    qa_results = QAResults()
    
    checks = [
        ("chronological_splits", assert_chronological_splits),
        ("feature_leakage", check_feature_leakage),
        ("e2e_pipeline", run_e2e_pipeline),
        ("label_conversion", validate_label_conversion),
        ("evaluation_metrics", review_evaluation_metrics)
    ]
    
    for check_name, check_func in checks:
        print(f"\nüìã Running check: {check_name}")
        try:
            status, evidence = check_func()
            qa_results.add_result(check_name, status, evidence)
            print(f"   Result: {status}")
            if status == "FAIL" and "error" in evidence:
                print(f"   Error: {evidence['error']}")
        except Exception as e:
            qa_results.add_result(check_name, "FAIL", {"error": str(e)})
            print(f"   Result: FAIL (Exception: {e})")
    
    if not qa_results.all_passed():
        print("\nüîß Applying auto-fixes...")
        qa_results = apply_auto_fixes(qa_results)
    
    print("\nüìÑ Generating QA report...")
    generate_qa_report(qa_results)
    
    print(f"\n{'='*50}")
    print(f"QA REVIEW SUMMARY")
    print(f"{'='*50}")
    
    for check_name, status in qa_results.results.items():
        status_icon = "‚úÖ" if status == "PASS" else "‚ùå"
        print(f"{status_icon} {check_name}: {status}")
    
    overall_status = "‚úÖ ALL PASS" if qa_results.all_passed() else "‚ùå SOME FAILURES"
    print(f"\nOverall: {overall_status}")
    print(f"Report saved to: artifacts/QA_REVIEW_REPORT.md")
    
    sys.exit(0 if qa_results.all_passed() else 1)

if __name__ == "__main__":
    main()
