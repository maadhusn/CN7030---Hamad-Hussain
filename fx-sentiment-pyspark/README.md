# Big-Data PySpark FX Sentiment Analysis

This is a Big-Data PySpark project for FX sentiment analysis using Delta Lake, MLflow, Feast feature store, and Streamlit UI. All pipelines are configured for maximum available history across FX/FRED/GDELT/Calendar data sources and designed for cluster deployment.

**Expected Data Volumes**: Qualify as big-data; run on cluster (Spark standalone/YARN/Databricks)  
**Training Target**: ‚â•70% accuracy with LR/RF/GBT/XGBoost and time-based CV (no guarantee; depends on data)  
**Architecture**: Medallion (Bronze ‚Üí Silver ‚Üí Gold) with unlimited data processing

## üèóÔ∏è Repo Map & Responsibilities

### Core Pipeline Components
- **code_spark/**: Big-data PySpark modules for unlimited history processing
  - `ingest_*.py`: Data ingestion jobs (FX, FRED, GDELT, Calendar) ‚Üí Delta Bronze partitioned by year/month/day
  - `silver_*.py`: Feature engineering and aggregation ‚Üí Delta Silver 1h bars
  - `gold_training_matrix.py`: Labeled training data with chronological splits
  - `train_big.py`: Enhanced model training (LR/RF/GBT/XGBoost) with chronological CV and calibration
  - `predict_batch.py`: Uncapped batch prediction over date ranges
  - `data_loader.py`: Configuration-driven data loading with big-data mode (no caps)
  - `models.py`, `evaluate.py`, `calibration.py`: ML pipeline components

### Legacy Components (Small-Scale Development)
- **bronze_ingest/**: Original collectors for development/testing
- **spark_jobs/**: Original processing jobs with row caps
- **configs/**: Legacy YAML configuration (superseded by conf/project.yaml)

### UI & Feature Store
- **apps/signal_ui/streamlit_app.py**: Signal dashboard reading artifacts/predictions/*.parquet and artifacts/metrics/*.csv
- **feature_repo/**: Feast feature store with Delta offline sources and SQLite online store

### Infrastructure
- **conf/project.yaml**: Single configuration file for big-data operations
- **Makefile**: Cluster-oriented targets for ingestion, training, and UI
- **docs/**: Deployment guides and data source documentation

## üîÑ Data Flow & Logic

### Bronze ‚Üí Silver Pipeline
1. **Data Ingestion**: `code_spark/ingest_*.py` ‚Üí Delta Bronze tables partitioned by date
   - FX: Alpha Vantage (daily) + TwelveData (1h) ‚Üí `bronze_fx`
   - FRED: Economic series (CPIAUCSL, PCEPI, UNRATE, FEDFUNDS, DTWEXBGS) ‚Üí `bronze_fred_series`
   - GDELT: Global Knowledge Graph sentiment ‚Üí `bronze_gkg`
   - Calendar: US economic release dates ‚Üí `bronze_fred_releases`

2. **Silver Processing**: Hourly aggregation and feature preparation
   - `silver_fx_features.py`: 1h EURUSD bars with technical indicators
   - `silver_us_calendar_1h.py`: Event flags and timing windows

### Features Engineering
Built in `spark_jobs/features_build.py` with no row caps in big-data mode:
- **Price Features**: Returns (1h, 3h, 6h, 12h, 24h), realized volatility (6h, 24h, 72h), EMA (Œ±=2/(w+1), 6h, 24h, 72h)
- **Sentiment Features**: GDELT tone (24h mean), event counts for keywords (CPI, inflation, payrolls, FOMC, ECB)
- **Macro Features**: FRED series with YoY/MoM transformations
- **Surprise Proxy**: Absolute deviation from 12-month rolling mean/median at event_0h anchor hours only
- **Event Flags**: `event_day_any`, `event_0h_any`, `pre_event_2h`, `post_event_2h`

### Labels & Training Logic
Generated in `spark_jobs/labels_make.py`:
- **3-Class Labels**: DOWN (-1), FLAT (0), UP (1) based on 1h forward returns
- **Dead-Zone**: Adaptive (40th percentile) or fixed (3 bps) threshold to handle noise
- **Chronological Split**: 70% train / 15% valid / 15% test with strict time ordering
- **Leakage Guards**: Assert max(train.ts) < min(valid.ts) < min(test.ts)

## ‚öôÔ∏è Configuration (No-Code Parameter Changes)

### Primary Configuration: conf/project.yaml
```yaml
mode: bigdata        # {bigdata|dev} - controls row caps
spark:
  shuffle_partitions: 400
  dynamic_allocation: true
data_ranges:
  fx:
    symbol: "EURUSD"
    start: null       # ALL available history
    end: null
  fred:
    series: ["CPIAUCSL","PCEPI"]
    start: null       # ALL available history
  gdelt:
    start: "2015-02-18T00:00:00Z"  # earliest reliable for GKG 2.1
training:
  model_candidates: ["lr","rf","gbt","xgb"]
  cv_folds: 5
  calibrate: true
limits:
  enabled: true      # dev mode caps
  max_rows: 1000000
big_run:
  enabled: false     # safety guard
  unlimited_data: true
```

### CLI Flag Overrides
- `--algorithms lr rf gbt`: Override model candidates
- `--output-dir artifacts/models/custom`: Change model save location
- `--start 2020-01-01 --end 2023-12-31`: Override date ranges

### Environment Overrides
- `BIG_RUN=1`: Enable unlimited data processing
- `MAX_TRAIN_ROWS=500000`: Override row caps in dev mode

## üîë API Keys & Secrets

### Environment & Secrets

1) Copy the template and fill in your keys:
   ```bash
   cp conf/secrets.env.example conf/secrets.env
   # edit conf/secrets.env and set your API keys and runtime flags
   ```

2) Makefile auto-loads `conf/secrets.env` for all commands.

3) To export in a plain shell:
   ```bash
   set -a; source conf/secrets.env; set +a
   ```

**Variables:**
- **ALPHAVANTAGE_API_KEY**, **TWELVEDATA_API_KEY**, **FRED_API_KEY**, **TRADINGECONOMICS_API_KEY**
- **BIG_RUN**, **ALLOW_BIG_RUN** (safety gates; keep false for local runs)
- **MAX_TRAIN_ROWS** (cap for local/dev runs)
- **Paths**: FEATURE_STORE_REGISTRY_PATH, DELTA_BASE, ARTIFACTS_DIR

### Getting Free API Keys
- **Alpha Vantage**: [Get free key](https://www.alphavantage.co/support/#api-key) (500 requests/day) - Powers daily FX data
- **TwelveData**: [Get free key](https://twelvedata.com/pricing) (800 requests/day) - Powers 1h FX bars
- **FRED**: [Get free key](https://fred.stlouisfed.org/docs/api/api_key.html) (optional) - Powers macro series

## üöÄ How to Run

### Local Development
```bash
# 1. Setup environment
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Configure secrets
cp conf/secrets.env.example conf/secrets.env
# Edit conf/secrets.env with your API keys

# 3. Set development mode
export BIG_RUN=0  # Safety guard

# 4. Run small-scale pipeline (legacy)
make train  # Uses spark_jobs/ with row caps

# 5. Run Streamlit UI
make ui-run  # streamlit run apps/signal_ui/streamlit_app.py --server.headless true
```

## üöÄ Run on Google Colab (15-Year Big Run)

### Quick Start (5 Steps)

1. **Open Colab Notebook**
   ```
   https://colab.research.google.com/github/maadhusn/CN7030---Hamad-Hussain/blob/colab-15y-run-ready/fx-sentiment-pyspark/notebooks/Colab_15Y_FX_Pipeline.ipynb
   ```

2. **Install Dependencies** (Cell 1)
   - Installs Java 17, Spark 3.5.0, Delta Lake, XGBoost4J-Spark
   - Runtime: ~3-5 minutes

3. **Setup Repository** (Cell 2)  
   - Clones repo and checks out `colab-15y-run-ready` branch
   - Runtime: ~1 minute

4. **Configure API Keys** (Cell 3)
   ```python
   # PASTE YOUR ACTUAL API KEYS HERE
   API_KEYS = {
       'ALPHAVANTAGE_API_KEY': 'your_alpha_vantage_key_here',  # ‚ö†Ô∏è REPLACE
       'TWELVEDATA_API_KEY': 'your_twelvedata_key_here',       # ‚ö†Ô∏è REPLACE  
       'FRED_API_KEY': 'your_fred_key_here',                   # ‚ö†Ô∏è REPLACE
   }
   ```

5. **Run Full Pipeline** (Cells 5-9)
   - **Expected Runtime**: 2-4 hours for 15 years of data
   - **Data Volume**: ~500MB-2GB depending on sources
   - **Models**: Logistic Regression, GBT, XGBoost (with fallback)

### Pipeline Stages

| Stage | Runtime | Output |
|-------|---------|--------|
| **Bronze Ingestion** | 30-60 min | Delta tables: `bronze_fx`, `bronze_fred_*`, `bronze_gkg`, `bronze_calendar_us` |
| **Silver Features** | 20-40 min | Technical indicators, macro features, sentiment scores |
| **Gold Matrix** | 10-20 min | Labeled training data with chronological splits |
| **Model Training** | 60-120 min | Trained models with performance metrics |
| **Visualizations** | 5-10 min | ROC curves, confusion matrices, calibration plots |

### Outputs Location

- **Models**: `/content/models/` (MLflow format)
- **Metrics**: `/content/reports/colab/*.json`
- **Plots**: `/content/reports/colab/*.png`
- **Final Report**: `/content/reports/colab/FINAL_PIPELINE_REPORT.md`

### Environment Overrides

**Date Range** (auto-calculated 15 years):
```python
os.environ['START_DATE'] = '2010-01-01'  # Override if needed
os.environ['END_DATE'] = '2025-01-01'    # Override if needed
```

**Development Mode** (90-day smoke test):
```python
# In Cell 4, set SMOKE_TEST = True for quick validation
SMOKE_TEST = True  # Uses 90-day range + row caps
```

### Anti-Leak Safeguards (Verified)

‚úÖ **Chronological Splits**: Train/validation/test respect time ordering  
‚úÖ **Proper Time Windows**: No future data leakage in features  
‚úÖ **Left-Only Joins**: Economic data joined with appropriate lags  
‚úÖ **Event Windows**: Pre/during/post event features properly aligned  

### Parameter Changes (No Code Edits)

All parameters configurable via `conf/project.yaml` or environment variables:

- **Date Ranges**: `START_DATE`/`END_DATE` environment variables
- **Row Limits**: `MAX_TRAIN_ROWS` environment variable  
- **Model Selection**: `conf/project.yaml` ‚Üí `training.model_candidates`
- **Feature Windows**: `conf/project.yaml` ‚Üí `features.*`

### Troubleshooting

**XGBoost Issues**: Pipeline automatically falls back to GBT if XGBoost4J-Spark fails  
**Memory Issues**: Reduce date range or enable row caps via `MAX_TRAIN_ROWS=100000`  
**API Rate Limits**: Built-in rate limiting and retry logic for all data sources  
**Delta Lake Issues**: Automatic fallback to Parquet format if Delta fails  

---

### Colab Deployment (Legacy)

### Cluster Deployment (Production)
```bash
# 1. Set cloud storage paths
export DELTA_PATH="s3a://your-bucket/delta"
export ARTIFACTS_PATH="s3a://your-bucket/artifacts"

# 2. Enable big-data mode
export BIG_RUN=1

# 3. Submit with XGBoost4J-Spark package
spark-submit \
  --packages ml.dmlc:xgboost4j-spark_2.12:1.7.6 \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --driver-memory 8g --executor-memory 16g \
  code_spark/train_big.py --algorithms lr rf gbt xgb --calibrate

# 4. Batch prediction (unlimited data)
make predict-batch  # Uses code_spark/predict_batch.py with no caps
```

## üéØ Training & Evaluation

### Algorithms & Features
- **Models**: XGBoost (default), LogisticRegression, RandomForest, GBTClassifier
- **Features Used**: 16-feature allow-list from price/sentiment/macro/event data
- **Cross-Validation**: Time-series splits with chronological ordering
- **Calibration**: Isotonic regression on validation scores
- **Threshold Tuning**: Maximize F1 score on validation set

### Hyperparameter Hints for ‚â•70% Accuracy
```python
# XGBoost4J-Spark (recommended)
xgb_params = {
    "max_depth": 6,
    "learning_rate": 0.1,
    "n_estimators": 100,
    "subsample": 0.8,
    "colsample_bytree": 0.8
}

# Logistic Regression (fallback)
lr_params = {
    "regParam": 0.01,
    "elasticNetParam": 0.5,
    "maxIter": 100
}
```

### Evaluation Metrics Produced
**Classification Metrics** (saved to `artifacts/metrics/*.csv`):
- Accuracy, Precision, Recall, F1 Score
- AUROC, AUPRC (Area Under Precision-Recall Curve)
- Log Loss, Brier Score (calibration quality)

**Visualizations** (saved to `artifacts/plots/`):
- ROC Curve, Precision-Recall Curve
- Calibration Plot (reliability diagram)
- Confusion Matrix (at 0.5 and best-F1 thresholds)
- Probability Histogram
- Feature Importance (tree-based models)
- Training vs Validation Learning Curves

**Regression Mode Metrics** (if applicable):
- RMSE, MAE, R¬≤
- RMSE¬≤ (squared RMSE for penalty emphasis)

## üìä Streamlit Signal UI

### Launch Command
```bash
streamlit run apps/signal_ui/streamlit_app.py --server.headless true
# Or: make ui-run
```

### Dashboard Features
- **Metrics Tab**: Model performance summary from `artifacts/metrics/*.csv`
- **Predictions Tab**: Recent predictions from `artifacts/predictions/*.parquet` with time-series plots
- **Performance Tab**: ROC/PR curves, confusion matrices, calibration plots
- **Model Info Tab**: Metadata, data sources, feature engineering summary

### Interactive Controls
- Classification threshold slider (0.0 - 1.0)
- Prediction row limit (100 - 10,000)
- Refresh data button

### Data Sources Read
- `artifacts/predictions/*.parquet`: Batch prediction results with ts, symbol, probability, label columns
- `artifacts/metrics/*.csv`: Training metrics with accuracy, f1, auc, precision, recall columns
- `artifacts/models/latest/metadata.json`: Model algorithm, timestamp, performance summary

## üçΩÔ∏è Optional: Feast Feature Store

### Setup (Development)
```bash
# Apply feature definitions
make feast-apply  # cd feature_repo && feast apply

# Materialize features (optional)
cd feature_repo && python materialize_online.py
```

### Configuration
- **Offline Store**: File-based pointing to Delta silver paths with no range restrictions
- **Online Store**: SQLite for development (Redis recommended for production)
- **Feature Views**: EURUSD hourly features with 16-feature allow-list

### Production Notes
- **Protobuf Compatibility**: Pin `protobuf>=3.20.0,<5.0.0` in requirements.txt
- **Cluster Materialization**: Use `feast materialize-incremental` on schedule
- **Online Serving**: Switch to Redis/DynamoDB for production workloads

### Workaround for Import Issues
If Feast import fails due to protobuf conflicts:
```bash
pip install protobuf==3.20.3  # Specific compatible version
# Or skip Feast integration - it's optional
```

## üîß Troubleshooting / FAQ

### Q: Do removed row caps cause data leakage?
**A**: No. Chronological splits with strict time ordering prevent leakage. The `split_timewise()` function enforces `max(train.ts) < min(valid.ts) < min(test.ts)` regardless of data volume.

### Q: How to tune for ‚â•70% accuracy?
**A**: 
1. Enable XGBoost with `--algorithms xgb` and cluster deployment
2. Increase feature engineering windows in `configs/features.yaml`
3. Adjust dead-zone threshold in label generation
4. Use more economic event keywords in calendar configuration
5. Extend data history by setting `start: null` in conf/project.yaml

### Q: Provider timezone/rate limit issues?
**A**:
- **Alpha Vantage**: 500 calls/day, UTC timestamps
- **TwelveData**: 800 calls/day, market timezone aware
- **FRED**: No key required but rate limits apply
- **GDELT**: 15-minute delay, UTC timestamps

### Q: Common runtime/import issues?
**A**:
```bash
# PySpark not found
export PYSPARK_PYTHON=python
export PYSPARK_DRIVER_PYTHON=python

# Delta Lake issues
pip install delta-spark==3.0.0

# Memory issues
export SPARK_DRIVER_MEMORY=8g
export SPARK_EXECUTOR_MEMORY=16g
```

## üìã Appendix: Public Function Signatures

### code_spark.train_big
```python
def prepare_spark() -> Tuple[SparkSession, bool]
def load_datasets() -> DataFrame
def build_feature_df() -> DataFrame
def train_model(algorithm: str = "lr") -> Dict[str, Any]
def evaluate_model(model, test_data) -> Dict[str, float]
def save_artifacts(results, output_dir: str = "artifacts/models/latest") -> str
def train_models(algorithms: Optional[List[str]] = None, calibrate: bool = True) -> Dict[str, Any]
def save_best_model(best: Dict[str, Any], outdir: str = "artifacts/models/latest") -> str
```

### code_spark.predict_batch
```python
def predict_batch(output_path: str, start_ts: str, end_ts: str, symbol: str = "EURUSD") -> str
```

### apps.signal_ui.streamlit_app
```python
def load_metrics_table() -> pd.DataFrame
def load_predictions(limit: int | None = None) -> pd.DataFrame
def compute_classification_metrics(y_true, y_score, threshold: float = 0.5) -> dict
def plot_confusion(y_true, y_score, threshold: float = 0.5, title: str = "") -> plt.Figure
def plot_roc(y_true, y_score, title: str = "") -> plt.Figure
def plot_pr(y_true, y_score, title: str = "") -> plt.Figure
def plot_calibration(y_true, y_score, title: str = "") -> plt.Figure
def build_app(test_mode=False) -> bool
```

### code_spark.data_loader
```python
def get_row_caps() -> Optional[Dict[str, int]]
def build_spark(app_name: str = "eurusd-ml") -> SparkSession
def get_feature_columns(df: DataFrame, label_col: str = "label", drop_cols: Optional[List[str]] = None, max_missing_frac: float = 0.20) -> List[str]
def split_timewise(df: DataFrame, ts_col: str = "ts", splits: Tuple[float, float, float] = (0.70, 0.15, 0.15), caps: Optional[Dict[str, int]] = None) -> Tuple[DataFrame, DataFrame, DataFrame]
```

---

**Note**: This is a production big-data project. For development/testing, use legacy `make train` with row caps. For production deployment, use `make train-big` on clusters with unlimited data processing.
