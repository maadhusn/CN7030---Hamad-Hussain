mode: bigdata        # {bigdata|dev}
spark:
  shuffle_partitions: 400
  dynamic_allocation: true
  default_parallelism: 400
big_run:
  enabled: true       # Enable unlimited data processing
  unlimited_data: true
limits:
  enabled: false      # Disable row caps for big runs
  max_rows: null      # No cap; code must handle None safely
data_ranges:
  fx:
    symbol: "EURUSD"
    start: "env:START_DATE"   # fallback: auto-15y (compute in code if env absent)
    end: "env:END_DATE"
  fred:
    series: ["CPIAUCSL","PCEPI"]   # extendable
    start: "env:START_DATE"   # fallback: auto-15y
    end: "env:END_DATE"
  gdelt:
    start: "2015-02-18T00:00:00Z"  # earliest reliable for GKG 2.1
    end: "env:END_DATE"
  calendar_us:
    start: "env:START_DATE"
    end: "env:END_DATE"
storage:
  base: "delta/"
  bronze: "delta/bronze"
  silver: "delta/silver"
  gold: "delta/gold"
features:
  allow_list_16: true
training:
  target: "label"
  positive_class: 1
  metrics: ["accuracy","auc","f1","precision","recall","logloss","brier"]
  model_candidates: ["lr","rf","gbt","xgb"]
  cv_folds: 5
  time_split: true
  calibrate: true
  max_train_rows: null  # No cap for big runs
