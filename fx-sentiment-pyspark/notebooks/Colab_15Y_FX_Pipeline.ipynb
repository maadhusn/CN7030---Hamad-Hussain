{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f20238",
   "metadata": {},
   "source": [
    "# üöÄ FX Sentiment Analysis: 15-Year Pipeline on Google Colab\n",
    "\n",
    "Complete end-to-end pipeline: **Bronze Ingestion ‚Üí Silver Features ‚Üí Gold Matrix ‚Üí Training ‚Üí Evaluation**\n",
    "\n",
    "**Expected Runtime**: 2-4 hours for 15 years of data  \n",
    "**Data Volume**: ~500MB-2GB depending on sources  \n",
    "**Models**: Logistic Regression, GBT, XGBoost (with fallback)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies (Runtime: ~3-5 minutes)\n",
    "print(\"üì¶ Installing Java 17, Spark 3.5.0, Delta Lake, XGBoost4J-Spark...\")\n",
    "\n",
    "!apt-get update -qq && apt-get install -y openjdk-17-jdk-headless -qq\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "\n",
    "!pip install pyspark==3.5.0 delta-spark==3.0.0 mlflow==2.8.1 -q\n",
    "!pip install scikit-learn==1.3.2 xgboost==2.0.3 plotly==5.17.0 -q\n",
    "!pip install pandas numpy requests python-dotenv pyarrow pyyaml tqdm matplotlib -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup Repository (Runtime: ~1 minute)\n",
    "print(\"üìÇ Setting up repository...\")\n",
    "\n",
    "!git clone https://github.com/maadhusn/CN7030---Hamad-Hussain.git /content/fx-pipeline\n",
    "%cd /content/fx-pipeline/fx-sentiment-pyspark\n",
    "!git checkout colab-15y-run-ready\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/fx-pipeline/fx-sentiment-pyspark')\n",
    "\n",
    "print(\"‚úÖ Repository setup complete!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8787cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configure API Keys\n",
    "print(\"üîë Configuring API keys...\")\n",
    "\n",
    "API_KEYS = {\n",
    "    'ALPHAVANTAGE_API_KEY': 'your_alpha_vantage_key_here',  # ‚ö†Ô∏è REPLACE\n",
    "    'TWELVEDATA_API_KEY': 'your_twelvedata_key_here',       # ‚ö†Ô∏è REPLACE  \n",
    "    'FRED_API_KEY': 'your_fred_key_here',                   # ‚ö†Ô∏è REPLACE\n",
    "    'TRADINGECONOMICS_API_KEY': '',                         # Optional\n",
    "}\n",
    "\n",
    "missing_keys = [k for k, v in API_KEYS.items() if not v and k != 'TRADINGECONOMICS_API_KEY']\n",
    "if missing_keys:\n",
    "    print(f\"‚ùå Missing required API keys: {missing_keys}\")\n",
    "    print(\"Please update the API_KEYS dictionary above with your actual keys.\")\n",
    "else:\n",
    "    with open('/content/fx-pipeline/fx-sentiment-pyspark/conf/secrets.env', 'w') as f:\n",
    "        for key, value in API_KEYS.items():\n",
    "            f.write(f\"{key}={value}\\n\")\n",
    "        f.write(\"BIG_RUN=true\\n\")\n",
    "        f.write(\"ALLOW_BIG_RUN=true\\n\")\n",
    "        f.write(\"MAX_TRAIN_ROWS=\\n\")  # No limit\n",
    "        f.write(\"DELTA_BASE=/content/delta\\n\")\n",
    "        f.write(\"ARTIFACTS_DIR=/content/artifacts\\n\")\n",
    "    \n",
    "    for key, value in API_KEYS.items():\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    print(\"‚úÖ API keys configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84177de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configure Pipeline (15-Year vs Smoke Test)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "SMOKE_TEST = False  # Set to True for quick validation\n",
    "\n",
    "if SMOKE_TEST:\n",
    "    print(\"üß™ SMOKE TEST MODE: 90-day validation run\")\n",
    "    end_date = datetime.utcnow().date()\n",
    "    start_date = end_date - timedelta(days=90)\n",
    "    os.environ['MAX_TRAIN_ROWS'] = '10000'\n",
    "else:\n",
    "    print(\"üöÄ FULL PIPELINE MODE: 15-year big data run\")\n",
    "    end_date = datetime.utcnow().date()\n",
    "    start_date = end_date.replace(day=1) - timedelta(days=365*15)\n",
    "    os.environ['MAX_TRAIN_ROWS'] = ''  # No limit\n",
    "\n",
    "os.environ['START_DATE'] = start_date.isoformat()\n",
    "os.environ['END_DATE'] = end_date.isoformat()\n",
    "os.environ['BIG_RUN'] = 'true'\n",
    "os.environ['ALLOW_BIG_RUN'] = 'true'\n",
    "\n",
    "print(f\"üìÖ Date Range: {start_date} to {end_date}\")\n",
    "print(f\"üìä Data Volume: {'~50MB (smoke)' if SMOKE_TEST else '~500MB-2GB (full)'}\")\n",
    "print(f\"‚è±Ô∏è Expected Runtime: {'~15-30 min' if SMOKE_TEST else '~2-4 hours'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initialize Spark Session\n",
    "print(\"‚ö° Initializing Spark with Delta Lake and XGBoost4J-Spark...\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "!mkdir -p /content/delta /content/models /content/reports/colab /content/artifacts\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FX-Sentiment-15Y-Pipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/content/delta\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/content/checkpoints\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0,ml.dmlc:xgboost4j-spark_2.12:2.0.3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark {spark.version} initialized successfully!\")\n",
    "print(f\"üìÇ Delta warehouse: /content/delta\")\n",
    "print(f\"üíæ Checkpoints: /content/checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Bronze Data Ingestion (Runtime: 30-60 min for full, 5-10 min for smoke)\n",
    "print(\"ü•â Starting Bronze Data Ingestion...\")\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ingestion_job(module_name, description):\n",
    "    print(f\"\\nüì• {description}...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['python', '-m', module_name],\n",
    "            cwd='/content/fx-pipeline/fx-sentiment-pyspark',\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=3600  # 1 hour timeout\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {description} completed in {elapsed:.1f}s\")\n",
    "        else:\n",
    "            print(f\"‚ùå {description} failed: {result.stderr}\")\n",
    "            return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"‚è∞ {description} timed out after 1 hour\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {description} error: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ingestion_jobs = [\n",
    "    ('code_spark.ingest_fx', 'FX Data Ingestion'),\n",
    "    ('code_spark.ingest_fred', 'FRED Economic Data Ingestion'),\n",
    "    ('code_spark.ingest_gdelt_gkg', 'GDELT News Sentiment Ingestion'),\n",
    "    ('code_spark.ingest_calendar_us', 'US Economic Calendar Ingestion'),\n",
    "]\n",
    "\n",
    "bronze_success = True\n",
    "for module, desc in ingestion_jobs:\n",
    "    if not run_ingestion_job(module, desc):\n",
    "        bronze_success = False\n",
    "        break\n",
    "\n",
    "if bronze_success:\n",
    "    print(\"\\nüéâ Bronze ingestion completed successfully!\")\n",
    "    print(\"üìä Delta tables created: bronze_fx, bronze_fred_*, bronze_gkg, bronze_calendar_us\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Bronze ingestion failed. Check logs above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cac3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Silver Feature Engineering (Runtime: 20-40 min for full, 3-5 min for smoke)\n",
    "print(\"ü•à Starting Silver Feature Engineering...\")\n",
    "\n",
    "if bronze_success:\n",
    "    silver_jobs = [\n",
    "        ('code_spark.silver_fx_features', 'FX Technical Indicators'),\n",
    "        ('code_spark.silver_us_calendar_1h', 'Economic Calendar Features'),\n",
    "    ]\n",
    "    \n",
    "    silver_success = True\n",
    "    for module, desc in silver_jobs:\n",
    "        if not run_ingestion_job(module, desc):\n",
    "            silver_success = False\n",
    "            break\n",
    "    \n",
    "    if silver_success:\n",
    "        print(\"\\nüéâ Silver feature engineering completed!\")\n",
    "        print(\"üìä Features: Technical indicators, macro features, sentiment scores\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Silver feature engineering failed.\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping silver features due to bronze ingestion failure.\")\n",
    "    silver_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Gold Training Matrix & Model Training (Runtime: 60-120 min for full, 10-15 min for smoke)\n",
    "print(\"ü•á Creating Gold Training Matrix & Training Models...\")\n",
    "\n",
    "if silver_success:\n",
    "    gold_success = run_ingestion_job('code_spark.gold_training_matrix', 'Gold Training Matrix Creation')\n",
    "    \n",
    "    if gold_success:\n",
    "        print(\"\\nü§ñ Starting Model Training...\")\n",
    "        training_success = run_ingestion_job('code_spark.train_big', 'Model Training (LR, GBT, XGBoost)')\n",
    "        \n",
    "        if training_success:\n",
    "            print(\"\\nüéâ Training completed successfully!\")\n",
    "            print(\"üìä Models trained: Logistic Regression, GBT, XGBoost (with fallback)\")\n",
    "            print(\"üíæ Models saved to: /content/models/\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Model training failed.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Gold matrix creation failed.\")\n",
    "        training_success = False\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping gold matrix and training due to silver feature failure.\")\n",
    "    training_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate Final Pipeline Report\n",
    "print(\"üìã Generating Final Pipeline Report...\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "report = {\n",
    "    \"pipeline_execution\": {\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"mode\": \"smoke_test\" if SMOKE_TEST else \"full_15y_run\",\n",
    "        \"date_range\": {\n",
    "            \"start\": os.environ.get('START_DATE'),\n",
    "            \"end\": os.environ.get('END_DATE')\n",
    "        },\n",
    "        \"stages\": {\n",
    "            \"bronze_ingestion\": bronze_success,\n",
    "            \"silver_features\": silver_success,\n",
    "            \"gold_matrix_training\": training_success\n",
    "        }\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"models\": \"/content/models/\",\n",
    "        \"metrics\": \"/content/reports/colab/\",\n",
    "        \"plots\": \"/content/reports/colab/\",\n",
    "        \"delta_tables\": \"/content/delta/\"\n",
    "    },\n",
    "    \"anti_leak_safeguards\": {\n",
    "        \"chronological_splits\": True,\n",
    "        \"proper_time_windows\": True,\n",
    "        \"left_only_joins\": True,\n",
    "        \"event_windows_aligned\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/content/reports/colab/FINAL_PIPELINE_REPORT.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "md_report = f\"\"\"# FX Sentiment Analysis Pipeline Report\n",
    "\n",
    "**Execution Time**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
    "**Mode**: {'90-Day Smoke Test' if SMOKE_TEST else '15-Year Full Pipeline'}\n",
    "**Date Range**: {os.environ.get('START_DATE')} to {os.environ.get('END_DATE')}\n",
    "\n",
    "\n",
    "| Stage | Status |\n",
    "|-------|--------|\n",
    "| Bronze Ingestion | {'‚úÖ SUCCESS' if bronze_success else '‚ùå FAILED'} |\n",
    "| Silver Features | {'‚úÖ SUCCESS' if silver_success else '‚ùå FAILED'} |\n",
    "| Gold Matrix & Training | {'‚úÖ SUCCESS' if training_success else '‚ùå FAILED'} |\n",
    "\n",
    "\n",
    "- **Models**: `/content/models/` (MLflow format)\n",
    "- **Metrics**: `/content/reports/colab/*.json`\n",
    "- **Plots**: `/content/reports/colab/*.png`\n",
    "- **Delta Tables**: `/content/delta/`\n",
    "\n",
    "\n",
    "- ‚úÖ Chronological splits maintained\n",
    "- ‚úÖ Proper time windows enforced\n",
    "- ‚úÖ Left-only joins with appropriate lags\n",
    "- ‚úÖ Event windows properly aligned\n",
    "\n",
    "\n",
    "{'Pipeline completed successfully! Models are ready for deployment.' if training_success else 'Review failed stages and check logs for troubleshooting.'}\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/reports/colab/FINAL_PIPELINE_REPORT.md', 'w') as f:\n",
    "    f.write(md_report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ FINAL PIPELINE REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(md_report)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "!find /content -name \"*.json\" -o -name \"*.png\" -o -name \"*.md\" | head -20\n",
    "\n",
    "if training_success:\n",
    "    print(\"\\nüéâ 15-Year FX Sentiment Analysis Pipeline completed successfully!\")\n",
    "    print(\"üìä Ready for model deployment and backtesting.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Pipeline completed with errors. Review logs above.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
